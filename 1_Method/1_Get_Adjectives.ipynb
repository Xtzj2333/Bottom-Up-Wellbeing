{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2922,
     "status": "ok",
     "timestamp": 1756331091736,
     "user": {
      "displayName": "Tony Liao",
      "userId": "13421491116072109086"
     },
     "user_tz": 240
    },
    "id": "x_YrfMEAyVbe",
    "outputId": "efba8011-3887-4b6a-c563-bcfd313b6191"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "from openai import OpenAI\n",
    "import time\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qTu6GCevm1t8"
   },
   "outputs": [],
   "source": [
    "here = '1_Method/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import spacy\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def _is_adjective(word: str) -> bool:\n",
    "    if not isinstance(word, str):\n",
    "        return False\n",
    "    return any(ss.pos() in (\"a\", \"s\") for ss in wn.synsets(word))\n",
    "\n",
    "\n",
    "def _group_by_lemma(words):\n",
    "    \"\"\"\n",
    "    Group adjectives by their lemma.\n",
    "    Skips multi-word phrases & any with punct.\n",
    "    \"\"\"\n",
    "\n",
    "    lemma_groups = defaultdict(list)\n",
    "    for word in words:\n",
    "        doc = nlp(word)\n",
    "        if doc and len(doc) > 0:\n",
    "            lemma = doc[0].lemma_\n",
    "            lemma_groups[lemma].append(word)\n",
    "    return lemma_groups\n",
    "\n",
    "    return dict(groups)\n",
    "\n",
    "def _group_by_prefix(words):\n",
    "    \"\"\"\n",
    "    Group adjectives by their first 4 characters.\n",
    "    Skips multi-word phrases & any with punct.\n",
    "    \"\"\"\n",
    "    groups = defaultdict(list)\n",
    "\n",
    "    for w in words:\n",
    "        key = w[:4]  # first 4 characters\n",
    "        groups[key].append(w)\n",
    "\n",
    "    return dict(groups)\n",
    "\n",
    "from typing import Dict, List\n",
    "\n",
    "def _pack_groups(groups: Dict[str, List[str]], pack_size: int = 5) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Return a list of lists where:\n",
    "      - any group with >1 word is kept as-is\n",
    "      - all singleton groups are merged into new lists of length `pack_size`\n",
    "        (the final one may be shorter)\n",
    "    \"\"\"\n",
    "    result: List[List[str]] = []\n",
    "    singletons: List[str] = []\n",
    "\n",
    "    for key, words in groups.items():\n",
    "        if len(words) > 1:\n",
    "            result.append(words)\n",
    "        elif len(words) == 1:\n",
    "            singletons.append(words[0])\n",
    "\n",
    "    # Pack the singletons\n",
    "    for i in range(0, len(singletons), pack_size):\n",
    "        result.append(singletons[i:i + pack_size])\n",
    "\n",
    "    return result\n",
    "\n",
    "def preprocess(words):\n",
    "  groups = []\n",
    "  for w in words:\n",
    "    w = w.lower()\n",
    "    if len(w.split()) > 1 and not w.isalpha() and not _is_adjective(w):\n",
    "        continue\n",
    "    groups.append(w)\n",
    "\n",
    "  groups = _group_by_lemma(groups)\n",
    "  return groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7e2AbjWcsHoC"
   },
   "source": [
    "## Load Candidate Adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Po30XecIFq7E"
   },
   "outputs": [],
   "source": [
    "# Vader & Evaluative Lexicon\n",
    "vader = pd.read_csv(here + \"data/dictionary/vader_lexicon.txt\", sep=\"\\t\", header=None)\n",
    "el = pd.read_excel(here + \"data/dictionary/evaluative_lexicon.xlsx\")\n",
    "\n",
    "# Theory driven words\n",
    "panas = 'interested, distressed, Excited, upset, strong, guilty, scared, hostile, enthusiastic, proud, irritable, alert, ashamed, inspired, nervous, determined, attentive, jittery, active, afraid'\n",
    "spane = 'positive, good, pleasant, joy, happy, contented, negative, bad, unpleasant, sad, afraid, angry'\n",
    "\n",
    "panas = panas.split(', ')\n",
    "spane = spane.split(', ')\n",
    "\n",
    "# gls\n",
    "gls = \"\"\"\n",
    "Interesting\n",
    "Reflective\n",
    "Bored\n",
    "Dull\n",
    "Unstimulated\n",
    "\n",
    "Happy\n",
    "Comfortable\n",
    "Pleasant\n",
    "Upbeat\n",
    "Sad\n",
    "Unhappy\n",
    "Unpleasant\n",
    "\n",
    "Fulfilled\n",
    "Coherent\n",
    "Meaningless\n",
    "Pointless\n",
    "\n",
    "Dramatic\n",
    "Engaging\n",
    "Uneventful\n",
    "Monotonous\n",
    "\n",
    "Enjoyable\n",
    "Unstable\n",
    "\n",
    "Disorganized\n",
    "Meaningful\n",
    "Purposeful\n",
    "Virtuous\n",
    "\"\"\"\n",
    "gls = gls.split('\\n')\n",
    "gls = [g.strip() for g in gls if g.strip()]\n",
    "\n",
    "prlq = \"\"\"\n",
    "Intense\n",
    "Unique\n",
    "Unusual\n",
    "Advanterous\n",
    "\"\"\"\n",
    "prlq = prlq.split('\\n')\n",
    "prlq = [p.strip() for p in prlq if p.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o9OZMeD_A_yy"
   },
   "outputs": [],
   "source": [
    "def _get_words(df):\n",
    "      # keep only alphabetic words\n",
    "    df = df[df.iloc[:, 0].apply(lambda x: str(x).isalpha())].copy()\n",
    "    df['pos'] = df.iloc[:, 0].apply(_is_adjective)\n",
    "\n",
    "    # get adjective words\n",
    "    words = df[df['pos']].iloc[:, 0].tolist()\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yq1dP15IBIO3"
   },
   "outputs": [],
   "source": [
    "vader_words = _get_words(vader)\n",
    "el_words = _get_words(el)\n",
    "\n",
    "new_words = vader_words + el_words + gls + prlq + spane + panas\n",
    "new_words = list(set([w.lower() for w in new_words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YBwv-uF2Hof9"
   },
   "outputs": [],
   "source": [
    "with open(here + 'data/Adjectives.pkl', 'wb') as f:\n",
    "  pickle.dump(new_words, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(here + 'data/Adjectives.pkl', 'rb') as f:\n",
    "    adjectives = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "def prompt(adj_list):\n",
    "    return f\"\"\"\n",
    "You will evaluate adjectives for whether they naturally describe a person's life when used in\n",
    "\"I want to live a ___ life\" or \"I don't want to live a ___ life.\"\n",
    "\n",
    "Given: {adj_list}\n",
    "\n",
    "Rules:\n",
    "- Return ONLY a comma-separated list of valid adjectives in lowercase, e.g., \"word x, word y, word z\".\n",
    "- If the words share a lemma, use BASE FORM (positive degree, lemma). No superlatives (e.g., use \"happy\" not \"happier/happiest\").\n",
    "- Each item must be a single adjective (no nouns, verbs, or multi-word terms) and it may include a natural hyphen.\n",
    "- Do not merge or remove the hyphen. Keep it as-is if it is a natural English adjective.\n",
    "- Must sound natural when used in a sentence right before the word \"life\". You should skip the adjectives\n",
    "    that do not make much sense when used right before the word \"life\" as a life descriptor.\n",
    "- If none are suitable, return \"none\".\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = {}\n",
    "\n",
    "for word in tqdm(adjectives):\n",
    "  prompt = prompt(word)\n",
    "\n",
    "  response = client.responses.create(\n",
    "        model=\"gpt-5-nano\",\n",
    "        input= prompt\n",
    "    )\n",
    "\n",
    "  print(prompt)\n",
    "  print(response.output_text)\n",
    "\n",
    "  processed[word] = response.output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(here + 'data/Adjective_Filtered.pkl', 'wb') as f:\n",
    "    pickle.dump(processed, f)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
